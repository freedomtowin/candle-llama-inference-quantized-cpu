[package]
name = "candle_llama_inference"
version = "0.2.0"
edition = "2021"

[dependencies]
tokio = { version = "1", features = ["full"] }
simple_logger = "1.11"
rand = "0.8"
toml = "0.5"
log = "0.4"
clap = { version = "4.1.8", features = ["derive"] }


candle-core = { git = "https://github.com/huggingface/candle.git", version = "0.5.1" }
candle-nn = { git = "https://github.com/huggingface/candle.git", version = "0.5.1" }
anyhow = "1.0.86"
serde = "1.0.203"
serde_json = "1.0.117"
tokenizers = {version = "0.19.1", features = ["onig"]}
candle-transformers = { git = "https://github.com/huggingface/candle.git", version = "0.5.1" }
hf-hub = "0.3.2"
tracing-subscriber = "0.3.18"
tracing-chrome = "0.7.2"
intel-mkl-src = {version = "0.8.1", optional = true}

[features]
default = []
accelerate = ["candle-core/accelerate", "candle-nn/accelerate", "candle-transformers/accelerate"]
mkl = ["dep:intel-mkl-src", "candle-core/mkl", "candle-nn/mkl", "candle-transformers/mkl"]
metal = ["candle-core/metal", "candle-nn/metal"]
